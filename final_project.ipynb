{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# all necessary imports\n",
    "import numpy as np \n",
    "import matplotlib.pyplot as plt\n",
    "import gym\n",
    "from stable_baselines3 import PPO\n",
    "from stable_baselines3.common.env_util import make_vec_env\n",
    "from gym import spaces\n",
    "import math"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ENVIRONMENT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomPongEnv(gym.Env):\n",
    "    metadata = {\"render_modes\": [\"human\"]}\n",
    "    \n",
    "    def __init__(self, reward_fn=None):\n",
    "        super().__init__()\n",
    "\n",
    "        # pass in reward function as it changes stage to stage\n",
    "        self.reward_fn = reward_fn \n",
    "\n",
    "        # initial game state\n",
    "        self.width, self.height = 1.0, 1.0\n",
    "        self.paddle_y = 0.5\n",
    "        \n",
    "        # action space: 0 = stay, 1 = up, 2 = down\n",
    "        self.action_space = spaces.Discrete(3)\n",
    "\n",
    "         # observation space: [ball_x, ball_y, ball_vx, ball_vy, paddle_y]\n",
    "        self.observation_space = spaces.Box(\n",
    "            low=0.0, high=1.0, shape=(5,), dtype=np.float32\n",
    "        )\n",
    "\n",
    "        self.episode_count = 0\n",
    "\n",
    "    def reset(self):\n",
    "        self.episode_count += 1      \n",
    "        return self.get_obs()\n",
    "    \n",
    "    def step(self, action):\n",
    "        if action == 1:\n",
    "            self.paddle_y += self.paddle_speed # move up\n",
    "        elif action == 2:\n",
    "            self.paddle_y -= self.paddle_speed # move down\n",
    "\n",
    "        # make sure paddle doesnt go off screen\n",
    "        self.paddle_y = np.clip(self.paddle_y, self.paddle_size /2, 1 - self.paddle_size /2) \n",
    "\n",
    "        ## move the ball\n",
    "        self.ball_x += self.ball_vx\n",
    "        self.ball_y += self.ball_vy\n",
    "\n",
    "        reward, done = self.reward_fn(self)\n",
    "\n",
    "        return self.get_obs(), reward, done, {}\n",
    "    \n",
    "    def get_obs(self):\n",
    "        return np.array([\n",
    "            self.ball_x,\n",
    "            self.ball_y,\n",
    "            self.ball_vx,\n",
    "            self.ball_vy,\n",
    "            self.paddle_y\n",
    "        ], dtype = np.float32)\n",
    "    \n",
    "    # initialize environment difficulty parameters    \n",
    "    def setDifficulty(self, ball_vx, ball_vy, paddle_size, ball_x, ball_y, paddle_speed):\n",
    "        self.paddle_size = paddle_size\n",
    "        self.ball_x = ball_x\n",
    "        self.ball_y = ball_y\n",
    "        self.ball_vx = ball_vx \n",
    "        self.ball_vy = ball_vy \n",
    "        self.paddle_speed = paddle_speed\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PPO + Curriculum Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from stable_baselines3.common.callbacks import BaseCallback\n",
    "\n",
    "# reward callback to track rewards during episodes\n",
    "class RewardCallback(BaseCallback):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.total_timesteps = 0\n",
    "        self.timesteps = []\n",
    "\n",
    "        self.cumulative_reward = 0\n",
    "        self.rewards = []\n",
    "        \n",
    "        self.current_episode = 0\n",
    "        self.episode_number = []\n",
    "\n",
    "        self.current_episode_reward = 0\n",
    "        self.episode_rewards = []\n",
    "        \n",
    "        self.current_episode_length = 0\n",
    "        self.episode_lengths = []\n",
    "        \n",
    "\n",
    "    def _on_step(self) -> bool:\n",
    "        # get current reward and done (if episode ends)\n",
    "        reward = self.locals[\"rewards\"][0]\n",
    "        done = self.locals[\"dones\"][0]\n",
    "\n",
    "        self.total_timesteps += 1\n",
    "        self.cumulative_reward += reward\n",
    "        self.current_episode_reward += reward\n",
    "        self.current_episode_length += 1\n",
    "\n",
    "        # save timestep metrics\n",
    "        self.rewards.append(self.cumulative_reward)\n",
    "        self.timesteps.append(self.total_timesteps)\n",
    "\n",
    "        if done: # end of episode\n",
    "\n",
    "            # save episode metrics\n",
    "            self.episode_rewards.append(self.current_episode_reward)\n",
    "            self.episode_number.append(self.current_episode)\n",
    "            self.episode_lengths.append(self.current_episode_length)\n",
    "\n",
    "            # reset for next episode\n",
    "            self.current_episode_reward = 0\n",
    "            self.current_episode_length = 0\n",
    "            self.current_episode += 1\n",
    "\n",
    "        return True\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Stage 1\n",
    "def stage1_reward(env):\n",
    "    reward = 0.01  # time alive bonus\n",
    "\n",
    "    # reward distance to the ball\n",
    "    reward += 0.1 * (1.0 - abs(env.paddle_y - env.ball_y))\n",
    "    \n",
    "    done = False\n",
    "    \n",
    "    ## ball hits top or bottom \n",
    "    if env.ball_y <= 0.0 or env.ball_y >= 1.0:\n",
    "        env.ball_vy *= -1\n",
    "\n",
    "    ## ball goes out of bounds on the right\n",
    "    if env.ball_x >= 1.0:\n",
    "        \n",
    "        # paddle hits ball!\n",
    "        if abs(env.ball_y - env.paddle_y) < env.paddle_size / 2:\n",
    "            reward += 1.0\n",
    "        \n",
    "        # paddle misses ball, episode ends\n",
    "        else:\n",
    "            reward -= 1\n",
    "            done = True\n",
    "\n",
    "            # reset environment to initial difficulty levels\n",
    "            env.setDifficulty(ball_vx=0, ball_vy=0.2, paddle_size=0.5, ball_x=1.0, ball_y=0.5, paddle_speed=0.3)\n",
    "\n",
    "    return reward, done\n",
    "\n",
    "def stage1(cycle_number):\n",
    "    # initialize new environment and set stage 1 difficulty \n",
    "    env1 = CustomPongEnv(reward_fn=stage1_reward)\n",
    "    callback_1 = RewardCallback()\n",
    "    env1.setDifficulty(ball_vx=0, ball_vy=0.2, paddle_size=0.5, ball_x=1.0, ball_y=0.5, paddle_speed=0.3)\n",
    "\n",
    "    # initialize new PPO model and start learning\n",
    "    model = PPO(\"MlpPolicy\", make_vec_env(lambda: env1, n_envs=1), verbose=1)\n",
    "    model.learn(total_timesteps=50000, callback=callback_1)\n",
    "\n",
    "    plt.figure(figsize=(20, 5))\n",
    "\n",
    "    # Plot 1: Cumulative Reward vs Timesteps\n",
    "    plt.subplot(1, 3, 1)\n",
    "    plt.plot(callback_1.timesteps, callback_1.rewards)\n",
    "    plt.xlabel(\"Timesteps\")\n",
    "    plt.ylabel(\"Cumulative Reward\")\n",
    "    plt.title(\"Cumulative Reward vs Timesteps\")\n",
    "    plt.grid(True)\n",
    "\n",
    "\n",
    "    # Plot 2: Episode Reward vs Episode Number\n",
    "    def moving_avg(data, window=500):\n",
    "        # makes plots smoother\n",
    "        return np.convolve(data, np.ones(window) / window, mode='valid')\n",
    "\n",
    "    smoothed_rewards = moving_avg(callback_1.episode_rewards)\n",
    "    smoothed_episodes = range(len(smoothed_rewards))  # optionally shift to align\n",
    "\n",
    "    plt.subplot(1, 3, 2)\n",
    "    step = 100  \n",
    "    plt.plot(smoothed_episodes[::step], smoothed_rewards[::step], markersize=2)\n",
    "    plt.xlabel(\"Episode\")\n",
    "    plt.ylabel(\"Total Reward\")\n",
    "    plt.title(\"Total Reward per Episode\")\n",
    "    plt.grid(True)\n",
    "\n",
    "    # Plot 3: Normalized Episode Reward vs Episode Number\n",
    "    normalized_rewards = []\n",
    "    for i in range(len(callback_1.episode_rewards)):\n",
    "        length = callback_1.episode_lengths[i]\n",
    "        reward = callback_1.episode_rewards[i]\n",
    "        normalized = reward / length if length > 0 else 0\n",
    "        normalized_rewards.append(normalized)\n",
    "\n",
    "    smoothed_normalized = moving_avg(normalized_rewards)\n",
    "    smoothed_episode_numbers = callback_1.episode_number[len(callback_1.episode_number) - len(smoothed_normalized):]\n",
    "\n",
    "    plt.subplot(1, 3, 3)\n",
    "    plt.plot(smoothed_episode_numbers[::step], smoothed_normalized[::step], markersize=2)\n",
    "    plt.xlabel(\"Episode\")\n",
    "    plt.ylabel(\"Reward Averaged over Timesteps\")\n",
    "    plt.title(\"Normalized Reward per Episode\")\n",
    "    plt.grid(True)\n",
    "    save_path = f\"Stage 1- {cycle_number}.png\"\n",
    "    plt.savefig(save_path)\n",
    "\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stage 2\n",
    "def stage2_reward(env):\n",
    "    reward = 0.01  # time alive bonus\n",
    "\n",
    "    # reward distance to the ball\n",
    "    dist = math.sqrt((env.paddle_y - env.ball_y) ** 2 + (1.0 - env.ball_x) ** 2)\n",
    "    max_dist = math.sqrt((1.0 - 0.0)**2 + (1.0 - 0.0)**2)  \n",
    "    normalized_dist = dist / max_dist\n",
    "    normalized_dist = np.clip(normalized_dist, 0.0, 1.0)\n",
    "    reward += 0.1 * (1.0 - normalized_dist)\n",
    "    \n",
    "    done = False\n",
    "    \n",
    "    # ball hits top or bottom\n",
    "    if env.ball_y <= 0.0 or env.ball_y >= 1.0:\n",
    "        env.ball_vy *= -1\n",
    "        env.ball_vx += np.random.uniform(-0.002, 0.002)  \n",
    "\n",
    "    # ball goes out of bounds on the right\n",
    "    if env.ball_x >= 1.0:\n",
    "\n",
    "        # paddle succesfully hits the ball back\n",
    "        if abs(env.ball_y - env.paddle_y) < env.paddle_size / 2:\n",
    "            reward += 1.0\n",
    "            env.ball_vx *= -1\n",
    "\n",
    "            # add random noise to the ball's velocity to introduce variability\n",
    "            env.ball_vx += np.random.uniform(-0.002, 0.002)  \n",
    "            env.ball_vy += np.random.uniform(-0.002, 0.002)  \n",
    "\n",
    "        # paddle misses ball, episode ends\n",
    "        else:\n",
    "            reward -= 1.0\n",
    "            done = True\n",
    "\n",
    "            # reset environment to initial stage 4 difficulty levels\n",
    "            env.setDifficulty(ball_vx=0.2, ball_vy=np.random.choice([-0.2, -0.1, 0.1, 0.2]), paddle_size=0.5, ball_x=0.5, ball_y=0.5, paddle_speed=0.3)\n",
    "\n",
    "    # ball hits left wall\n",
    "    if env.ball_x <= 0.0:\n",
    "        env.ball_vx *= -1\n",
    "        env.ball_vy += np.random.uniform(-0.002, 0.002)  \n",
    "\n",
    "    return reward, done\n",
    "\n",
    "def stage2(cycle_number, model):\n",
    "    # initialize new environment and set stage 2 difficulty \n",
    "    callback_2 = RewardCallback()\n",
    "    env2 = CustomPongEnv(reward_fn=stage2_reward)\n",
    "    env2.setDifficulty(ball_vx=0.2, ball_vy=0.2, paddle_size=0.5, ball_x=0.5, ball_y=0.5, paddle_speed=0.3)\n",
    "    \n",
    "    # use stage 1 ppo model to continue learning on environment 2\n",
    "    model.set_env(make_vec_env(lambda: env2, n_envs=1))\n",
    "    model.learn(total_timesteps=100000, callback=callback_2)\n",
    "\n",
    "    plt.figure(figsize=(20, 5))\n",
    "\n",
    "    # Plot 1: Cumulative Reward vs Timesteps\n",
    "    plt.subplot(1, 3, 1)\n",
    "    plt.plot(callback_2.timesteps, callback_2.rewards)\n",
    "    plt.xlabel(\"Timesteps\")\n",
    "    plt.ylabel(\"Cumulative Reward\")\n",
    "    plt.title(\"Cumulative Reward vs Timesteps\")\n",
    "    plt.grid(True)\n",
    "\n",
    "    # Plot 2: Episode Reward vs Episode Number\n",
    "    def moving_avg(data, window=500):\n",
    "        # makes plots smoother\n",
    "        return np.convolve(data, np.ones(window) / window, mode='valid')\n",
    "\n",
    "    smoothed_rewards = moving_avg(callback_2.episode_rewards)\n",
    "    smoothed_episodes = range(len(smoothed_rewards)) \n",
    "\n",
    "    plt.subplot(1, 3, 2)\n",
    "    step = 100 # plot every 100 steps\n",
    "    plt.plot(smoothed_episodes[::step], smoothed_rewards[::step], markersize=2)\n",
    "    plt.xlabel(\"Episode\")\n",
    "    plt.ylabel(\"Total Reward\")\n",
    "    plt.title(\"Total Reward per Episode\")\n",
    "    plt.grid(True)\n",
    "\n",
    "    # Plot 3: Normalized Episode Reward vs Episode Number\n",
    "    normalized_rewards = []\n",
    "    for i in range(len(callback_2.episode_rewards)):\n",
    "        length = callback_2.episode_lengths[i]\n",
    "        reward = callback_2.episode_rewards[i]\n",
    "        normalized = reward / length if length > 0 else 0\n",
    "        normalized_rewards.append(normalized)\n",
    "\n",
    "    smoothed_normalized = moving_avg(normalized_rewards)\n",
    "    smoothed_episode_numbers = callback_2.episode_number[len(callback_2.episode_number) - len(smoothed_normalized):]\n",
    "\n",
    "    plt.subplot(1, 3, 3)\n",
    "    plt.plot(smoothed_episode_numbers[::step], smoothed_normalized[::step], markersize=2)\n",
    "    plt.xlabel(\"Episode\")\n",
    "    plt.ylabel(\"Reward Averaged over Timesteps\")\n",
    "    plt.title(\"Normalized Reward per Episode\")\n",
    "    plt.grid(True)\n",
    "\n",
    "    save_path = f\"Stage 2- {cycle_number}.png\"\n",
    "    plt.savefig(save_path)\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stage 3\n",
    "def stage3_reward(env):\n",
    "    reward = 0.01  # time alive bonus\n",
    "\n",
    "    # reward distance to the ball\n",
    "    dist = math.sqrt((env.paddle_y - env.ball_y) ** 2 + (1.0 - env.ball_x) ** 2)\n",
    "    max_dist = math.sqrt((1.0 - 0.0)**2 + (1.0 - 0.0)**2)  \n",
    "    normalized_dist = dist / max_dist\n",
    "    normalized_dist = np.clip(normalized_dist, 0.0, 1.0)\n",
    "    reward += 0.1 * (1.0 - normalized_dist)\n",
    "    \n",
    "    done = False\n",
    "    # ball hits top or bottom\n",
    "    if env.ball_y <= 0.0 or env.ball_y >= 1.0:\n",
    "        env.ball_vy *= -1\n",
    "        env.ball_vx += np.random.uniform(-0.002, 0.002)  \n",
    "\n",
    "    # ball goes out of bounds on the right\n",
    "    if env.ball_x >= 1.0:\n",
    "\n",
    "        # paddle succesfully hits ball back\n",
    "        if abs(env.ball_y - env.paddle_y) < env.paddle_size / 2:\n",
    "            reward += 0.2\n",
    "            env.ball_vx *= -1\n",
    "            \n",
    "            # add random noise to the ball's velocity to introduce variability\n",
    "            env.ball_vx += np.random.uniform(-0.002, 0.002)  \n",
    "            env.ball_vy += np.random.uniform(-0.002, 0.002)  \n",
    "\n",
    "        # paddle misses ball, episode ends\n",
    "        else:\n",
    "            reward -= 0.2\n",
    "            done = True\n",
    "\n",
    "            # reset environment to initial stage 3 difficulty levels\n",
    "            env.setDifficulty(ball_vx=np.random.choice([-0.2, -0.1, 0.1, 0.2]), ball_vy=np.random.choice([-0.2, -0.1, 0.1, 0.2]), paddle_size=0.5, ball_x=0.5, ball_y=0.5, paddle_speed=0.3)\n",
    "\n",
    "    # ball hits left wall\n",
    "    if env.ball_x <= 0.0:\n",
    "\n",
    "        env.ball_vx *= -1\n",
    "        env.ball_vy += np.random.uniform(-0.002, 0.002)  \n",
    "\n",
    "    return reward, done\n",
    "\n",
    "def stage3(cycle_number, model):\n",
    "    # initialize new environment and set stage 3 difficulty \n",
    "    callback_3 = RewardCallback()\n",
    "    env3 = CustomPongEnv(reward_fn=stage3_reward)\n",
    "    env3.setDifficulty(ball_vx=0.2, ball_vy=0.2, paddle_size=0.5, ball_x=0.5, ball_y=0.5, paddle_speed=0.3)\n",
    "\n",
    "    # use stage 2 ppo model to continue learning on environment 3\n",
    "    model.set_env(make_vec_env(lambda: env3, n_envs=1))\n",
    "    model.learn(total_timesteps=150000, callback=callback_3)\n",
    "\n",
    "    plt.figure(figsize=(20, 5))\n",
    "\n",
    "    # Plot 1: Cumulative Reward vs Timesteps\n",
    "    plt.subplot(1, 3, 1)\n",
    "    plt.plot(callback_3.timesteps, callback_3.rewards)\n",
    "    plt.xlabel(\"Timesteps\")\n",
    "    plt.ylabel(\"Cumulative Reward\")\n",
    "    plt.title(\"Cumulative Reward vs Timesteps\")\n",
    "    plt.grid(True)\n",
    "\n",
    "    # Plot 2: Episode Reward vs Episode Number\n",
    "    def moving_avg(data, window=50):\n",
    "        # makes plots smoother\n",
    "        return np.convolve(data, np.ones(window) / window, mode='valid')\n",
    "\n",
    "    smoothed_rewards = moving_avg(callback_3.episode_rewards)\n",
    "    smoothed_episodes = range(len(smoothed_rewards)) \n",
    "\n",
    "    plt.subplot(1, 3, 2)\n",
    "    step = 25  # plot every 25 steps\n",
    "    plt.plot(smoothed_episodes[::step], smoothed_rewards[::step], markersize=2)\n",
    "    plt.xlabel(\"Episode\")\n",
    "    plt.ylabel(\"Total Reward\")\n",
    "    plt.title(\"Total Reward per Episode\")\n",
    "    plt.grid(True)\n",
    "\n",
    "    # Plot 3: Normalized Episode Reward vs Episode Number\n",
    "    normalized_rewards = []\n",
    "    for i in range(len(callback_3.episode_rewards)):\n",
    "        length = callback_3.episode_lengths[i]\n",
    "        reward = callback_3.episode_rewards[i]\n",
    "        normalized = reward / length if length > 0 else 0\n",
    "        normalized_rewards.append(normalized)\n",
    "\n",
    "    smoothed_normalized = moving_avg(normalized_rewards)\n",
    "    smoothed_episode_numbers = callback_3.episode_number[len(callback_3.episode_number) - len(smoothed_normalized):]\n",
    "\n",
    "    plt.subplot(1, 3, 3)\n",
    "    plt.plot(smoothed_episode_numbers[::step], smoothed_normalized[::step], markersize=2)\n",
    "    plt.xlabel(\"Episode\")\n",
    "    plt.ylabel(\"Reward Averaged over Timesteps\")\n",
    "    plt.title(\"Normalized Reward per Episode\")\n",
    "    plt.grid(True)\n",
    "    save_path = f\"Stage 3- {cycle_number}.png\"\n",
    "    plt.savefig(save_path)\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stage 4\n",
    "def stage4_reward(env):\n",
    "    reward = 0.01  # time alive bonus\n",
    "\n",
    "    # reward distance to the ball\n",
    "    dist = math.sqrt((env.paddle_y - env.ball_y) ** 2 + (1.0 - env.ball_x) ** 2)\n",
    "    max_dist = math.sqrt((1.0 - 0.0)**2 + (1.0 - 0.0)**2)  \n",
    "    normalized_dist = dist / max_dist\n",
    "    normalized_dist = np.clip(normalized_dist, 0.0, 1.0)\n",
    "    reward += 0.1 * (1.0 - normalized_dist)\n",
    "    \n",
    "    done = False\n",
    "\n",
    "    # ball hits top or bottom\n",
    "    if env.ball_y <= 0.0 or env.ball_y >= 1.0:\n",
    "        env.ball_vy *= -1\n",
    "        env.ball_vx += np.random.uniform(-0.002, 0.002)  \n",
    "\n",
    "    # ball goes out of bounds on the right\n",
    "    if env.ball_x >= 1.0:\n",
    "        \n",
    "        # paddle succesfully hits ball back\n",
    "        if abs(env.ball_y - env.paddle_y) < env.paddle_size / 2:\n",
    "            reward += 0.2\n",
    "            env.ball_vx *= -1\n",
    "\n",
    "            # add random noise to the ball's velocity to introduce variability\n",
    "            env.ball_vx += np.random.uniform(-0.002, 0.002)  \n",
    "            env.ball_vy += np.random.uniform(-0.002, 0.002) \n",
    "             \n",
    "        # paddle misses ball, episode ends            \n",
    "        else:\n",
    "            reward -= 0.2\n",
    "            done = True\n",
    "\n",
    "            # reset environment to initial stage 4 difficulty levels\n",
    "            env.setDifficulty(ball_vx=np.random.choice([-0.2, -0.1, 0.1, 0.2]), ball_vy=np.random.choice([-0.2, -0.1, 0.1, 0.2]), paddle_size=0.5, ball_x=np.random.choice([ 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9]), ball_y=np.random.choice([ 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9]), paddle_speed=0.3)\n",
    "\n",
    "    # ball hits left wall\n",
    "    if env.ball_x <= 0.0:\n",
    "        env.ball_vx *= -1\n",
    "        env.ball_vy += np.random.uniform(-0.002, 0.002)  \n",
    "\n",
    "    return reward, done\n",
    "\n",
    "def stage4(cycle_number, model):\n",
    "    # initialize new environment and set stage 4 difficulty \n",
    "    callback_4 = RewardCallback()\n",
    "    env4 = CustomPongEnv(reward_fn=stage4_reward)\n",
    "    env4.setDifficulty(ball_vx=0.2, ball_vy=0.2, paddle_size=0.5, ball_x=0.5, ball_y=0.5, paddle_speed=0.3)\n",
    "\n",
    "    # use stage 3 ppo model to continue learning on environment 4 \n",
    "    model.set_env(make_vec_env(lambda: env4, n_envs=1))\n",
    "    model.learn(total_timesteps=150000, callback=callback_4)\n",
    "\n",
    "    plt.figure(figsize=(20, 5))\n",
    "\n",
    "    # Plot 1: Cumulative Reward vs Timesteps\n",
    "    plt.subplot(1, 3, 1)\n",
    "    plt.plot(callback_4.timesteps, callback_4.rewards)\n",
    "    plt.xlabel(\"Timesteps\")\n",
    "    plt.ylabel(\"Cumulative Reward\")\n",
    "    plt.title(\"Cumulative Reward vs Timesteps\")\n",
    "    plt.grid(True)\n",
    "\n",
    "    # Plot 2: Episode Reward vs Episode Number\n",
    "    def moving_avg(data, window=50):\n",
    "        # makes plots smoother\n",
    "        return np.convolve(data, np.ones(window) / window, mode='valid')\n",
    "\n",
    "    smoothed_rewards = moving_avg(callback_4.episode_rewards)\n",
    "    smoothed_episodes = range(len(smoothed_rewards)) \n",
    "    plt.subplot(1, 3, 2)\n",
    "    step = 25  # plot every 25 steps\n",
    "    plt.plot(smoothed_episodes[::step], smoothed_rewards[::step], markersize=2)\n",
    "    plt.xlabel(\"Episode\")\n",
    "    plt.ylabel(\"Total Reward\")\n",
    "    plt.title(\"Total Reward per Episode\")\n",
    "    plt.grid(True)\n",
    "\n",
    "    # Plot 3: Normalized Episode Reward vs Episode Number\n",
    "    normalized_rewards = []\n",
    "    for i in range(len(callback_4.episode_rewards)):\n",
    "        length = callback_4.episode_lengths[i]\n",
    "        reward = callback_4.episode_rewards[i]\n",
    "        normalized = reward / length if length > 0 else 0\n",
    "        normalized_rewards.append(normalized)\n",
    "\n",
    "    smoothed_normalized = moving_avg(normalized_rewards)\n",
    "    smoothed_episode_numbers = callback_4.episode_number[len(callback_4.episode_number) - len(smoothed_normalized):]\n",
    "\n",
    "    plt.subplot(1, 3, 3)\n",
    "    plt.plot(smoothed_episode_numbers[::step], smoothed_normalized[::step], markersize=2)\n",
    "    plt.xlabel(\"Episode\")\n",
    "    plt.ylabel(\"Reward Averaged over Timesteps\")\n",
    "    plt.title(\"Normalized Reward per Episode\")\n",
    "    plt.grid(True)\n",
    "    save_path = f\"Stage 4- {cycle_number}.png\"\n",
    "    plt.savefig(save_path)\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stage 5\n",
    "def stage5_reward(env):\n",
    "    reward = 0.01  # time alive bonus\n",
    "\n",
    "    # reward distance to the ball\n",
    "    dist = math.sqrt((env.paddle_y - env.ball_y) ** 2 + (1.0 - env.ball_x) ** 2)\n",
    "    max_dist = math.sqrt((1.0 - 0.0)**2 + (1.0 - 0.0)**2)  # ~1.414\n",
    "    normalized_dist = dist / max_dist\n",
    "    normalized_dist = np.clip(normalized_dist, 0.0, 1.0)\n",
    "    reward += 0.1 * (1.0 - normalized_dist)\n",
    "    \n",
    "    done = False\n",
    "    \n",
    "    # ball hits top or bottom\n",
    "    if env.ball_y <= 0.0 or env.ball_y >= 1.0:\n",
    "        env.ball_vy *= -1\n",
    "        env.ball_vx += np.random.uniform(-0.002, 0.002)  \n",
    "\n",
    "     # ball goes out of bounds on the right\n",
    "    if env.ball_x >= 1.0:\n",
    "        \n",
    "        # paddle succesfully hits ball back\n",
    "        if abs(env.ball_y - env.paddle_y) < env.paddle_size / 2:\n",
    "            reward += 0.2\n",
    "            env.ball_vx *= -1\n",
    "\n",
    "            # add random noise to the ball's velocity to introduce variability\n",
    "            env.ball_vx += np.random.uniform(-0.002, 0.002)  \n",
    "            env.ball_vy += np.random.uniform(-0.002, 0.002)  \n",
    "        \n",
    "        # paddle misses ball, episode ends       \n",
    "        else:\n",
    "            reward -= 0.2\n",
    "            done = True\n",
    "\n",
    "            # every 100 episodes, decrease paddle size by 0.05\n",
    "            if (env.episode_count % 100 == 0):\n",
    "                env.paddle_size -= 0.05\n",
    "\n",
    "            # reset environment to initial stage 5 difficulty levels\n",
    "            env.setDifficulty(ball_vx=np.random.choice([-0.2, -0.1, 0.1, 0.2]), ball_vy=np.random.choice([-0.2, -0.1, 0.1, 0.2]), paddle_size=env.paddle_size, ball_x=np.random.choice([0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9]), ball_y=np.random.choice([0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9]), paddle_speed=0.3)\n",
    "\n",
    "    # ball hits left wall\n",
    "    if env.ball_x <= 0.0:\n",
    "        env.ball_vx *= -1\n",
    "        env.ball_vy += np.random.uniform(-0.002, 0.002)  \n",
    "\n",
    "    return reward, done\n",
    "\n",
    "def stage5(cycle_number, model):\n",
    "    # initialize new environment and set stage 5 difficulty \n",
    "    callback_5 = RewardCallback()\n",
    "    env5 = CustomPongEnv(reward_fn=stage5_reward)\n",
    "    env5.setDifficulty(ball_vx=0.2, ball_vy=0.2, paddle_size=0.5, ball_x=0.5, ball_y=0.5, paddle_speed=0.3)\n",
    "\n",
    "    # use stage 4 ppo model to continue learning on environment 5\n",
    "    model.set_env(make_vec_env(lambda: env5, n_envs=1))\n",
    "    model.learn(total_timesteps=150000, callback=callback_5)\n",
    "\n",
    "    plt.figure(figsize=(20, 5))\n",
    "\n",
    "    # Plot 1: Cumulative Reward vs Timesteps\n",
    "    plt.subplot(1, 3, 1)\n",
    "    plt.plot(callback_5.timesteps, callback_5.rewards)\n",
    "    plt.xlabel(\"Timesteps\")\n",
    "    plt.ylabel(\"Cumulative Reward\")\n",
    "    plt.title(\"Cumulative Reward vs Timesteps\")\n",
    "    plt.grid(True)\n",
    "\n",
    "    # Plot 2: Episode Reward vs Episode Number\n",
    "    def moving_avg(data, window=20):\n",
    "        # makes plots smoother\n",
    "        return np.convolve(data, np.ones(window) / window, mode='valid')\n",
    "\n",
    "    smoothed_rewards = moving_avg(callback_5.episode_rewards)\n",
    "    smoothed_episodes = range(len(smoothed_rewards)) \n",
    "\n",
    "    plt.subplot(1, 3, 2)\n",
    "    step = 25  # plot every 25 steps\n",
    "    plt.plot(smoothed_episodes[::step], smoothed_rewards[::step], markersize=2)\n",
    "    plt.xlabel(\"Episode\")\n",
    "    plt.ylabel(\"Total Reward\")\n",
    "    plt.title(\"Total Reward per Episode\")\n",
    "    plt.grid(True)\n",
    "\n",
    "    # Plot 3: Normalized Episode Reward vs Episode Number\n",
    "    normalized_rewards = []\n",
    "    for i in range(len(callback_5.episode_rewards)):\n",
    "        length = callback_5.episode_lengths[i]\n",
    "        reward = callback_5.episode_rewards[i]\n",
    "        normalized = reward / length if length > 0 else 0\n",
    "        normalized_rewards.append(normalized)\n",
    "\n",
    "    smoothed_normalized = moving_avg(normalized_rewards)\n",
    "    smoothed_episode_numbers = callback_5.episode_number[len(callback_5.episode_number) - len(smoothed_normalized):]\n",
    "\n",
    "    plt.subplot(1, 3, 3)\n",
    "    plt.plot(smoothed_episode_numbers[::step], smoothed_normalized[::step], markersize=2)\n",
    "    plt.xlabel(\"Episode\")\n",
    "    plt.ylabel(\"Reward Averaged over Timesteps\")\n",
    "    plt.title(\"Normalized Reward per Episode\")\n",
    "    plt.grid(True)\n",
    "\n",
    "    save_path = f\"Stage 5- {cycle_number}.png\"\n",
    "    plt.savefig(save_path)\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run stages in order times to get best plots\n",
    "for i in range(1):\n",
    "    model = stage1(i)\n",
    "    model = stage2(i, model)\n",
    "    model = stage3(i, model)\n",
    "    model = stage4(i, model)\n",
    "    model = stage5(i, model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PPO Only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ppo_reward(env):\n",
    "    reward = 0 \n",
    "    \n",
    "    done = False\n",
    "    \n",
    "    # ball hits top or bottom\n",
    "    if env.ball_y <= 0.0 or env.ball_y >= 1.0:\n",
    "        env.ball_vy *= -1\n",
    "        env.ball_vx += np.random.uniform(-0.002, 0.002)  \n",
    "\n",
    "    # ball goes out of bounds on the right\n",
    "    if env.ball_x >= 1.0:\n",
    "\n",
    "        # paddle succesfully hits ball back\n",
    "        if abs(env.ball_y - env.paddle_y) < env.paddle_size / 2:\n",
    "            reward += 1.0\n",
    "            env.ball_vx *= -1\n",
    "\n",
    "            # add random noise to the ball's velocity to introduce variability\n",
    "            env.ball_vx += np.random.uniform(-0.002, 0.002)  \n",
    "            env.ball_vy += np.random.uniform(-0.002, 0.002)  \n",
    "\n",
    "        # paddle misses ball, episode ends\n",
    "        else:\n",
    "            reward -= 1.0\n",
    "            done = True\n",
    "\n",
    "            # reset environment to initial difficulty levels\n",
    "            env.setDifficulty(ball_vx=np.random.choice([-0.2, -0.1, 0.1, 0.2]), ball_vy=np.random.choice([-0.2, -0.1, 0.1, 0.2]), paddle_size=0.3, ball_x=np.random.choice([0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9]), ball_y=np.random.choice([0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9]), paddle_speed=0.3)\n",
    "\n",
    "    # ball hits left wall\n",
    "    if env.ball_x <= 0.0:\n",
    "        env.ball_vx *= -1\n",
    "        env.ball_vy += np.random.uniform(-0.002, 0.002)  \n",
    "\n",
    "    return reward, done\n",
    "\n",
    "\n",
    "# initialize new pong environment and set difficulty\n",
    "ppo_callback = RewardCallback()\n",
    "ppo_env = CustomPongEnv(reward_fn=ppo_reward)\n",
    "ppo_env.setDifficulty(ball_vx=0.2, ball_vy=0.2, paddle_size=0.3, ball_x=0.5, ball_y=0.5, paddle_speed=0.3)\n",
    "\n",
    "# initialize new PPO model and learn for 600,000 timesteps\n",
    "model = PPO(\"MlpPolicy\", make_vec_env(lambda: ppo_env, n_envs=1), verbose=1)\n",
    "model.learn(total_timesteps=600000, callback=ppo_callback)\n",
    "\n",
    "plt.figure(figsize=(20, 5))\n",
    "\n",
    "# Plot 1: Cumulative Reward vs Timesteps\n",
    "plt.subplot(1, 3, 1)\n",
    "plt.plot(ppo_callback.timesteps, ppo_callback.rewards)\n",
    "plt.xlabel(\"Timesteps\")\n",
    "plt.ylabel(\"Cumulative Reward\")\n",
    "plt.title(\"Cumulative Reward vs Timesteps\")\n",
    "plt.grid(True)\n",
    "\n",
    "# Plot 2: Episode Reward vs Episode Number\n",
    "def moving_avg(data, window=500):\n",
    "    # makes plots smoother\n",
    "    return np.convolve(data, np.ones(window) / window, mode='valid')\n",
    "\n",
    "smoothed_rewards = moving_avg(ppo_callback.episode_rewards)\n",
    "smoothed_episodes = range(len(smoothed_rewards))  \n",
    "\n",
    "plt.subplot(1, 3, 2)\n",
    "step = 100 \n",
    "plt.plot(smoothed_episodes[::step], smoothed_rewards[::step], markersize=2)\n",
    "plt.xlabel(\"Episode\")\n",
    "plt.ylabel(\"Total Reward\")\n",
    "plt.title(\"Total Reward per Episode\")\n",
    "plt.grid(True)\n",
    "\n",
    "# Plot 3: Normalized Episode Reward vs Episode Number\n",
    "normalized_rewards = []\n",
    "for i in range(len(ppo_callback.episode_rewards)):\n",
    "    length = ppo_callback.episode_lengths[i]\n",
    "    reward = ppo_callback.episode_rewards[i]\n",
    "    normalized = reward / length if length > 0 else 0\n",
    "    normalized_rewards.append(normalized)\n",
    "\n",
    "smoothed_normalized = moving_avg(normalized_rewards)\n",
    "smoothed_episode_numbers = ppo_callback.episode_number[len(ppo_callback.episode_number) - len(smoothed_normalized):]\n",
    "\n",
    "plt.subplot(1, 3, 3)\n",
    "plt.plot(smoothed_episode_numbers[::step], smoothed_normalized[::step], markersize=2)\n",
    "plt.xlabel(\"Episode\")\n",
    "plt.ylabel(\"Reward Averaged over Timesteps\")\n",
    "plt.title(\"Normalized Reward per Episode\")\n",
    "plt.grid(True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
